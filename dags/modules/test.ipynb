{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-airflow in c:\\users\\personal\\anaconda3\\lib\\site-packages (2.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\personal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: alembic<2.0,>=1.13.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.14.0)\n",
      "Requirement already satisfied: argcomplete>=1.10 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.5.1)\n",
      "Requirement already satisfied: asgiref>=2.3.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.8.1)\n",
      "Requirement already satisfied: attrs>=22.1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (23.1.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.6.2)\n",
      "Requirement already satisfied: colorlog>=6.8.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (6.9.0)\n",
      "Requirement already satisfied: configupdater>=3.1.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.2)\n",
      "Requirement already satisfied: connexion<3.0,>=2.14.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from connexion[flask]<3.0,>=2.14.2->apache-airflow) (2.14.2)\n",
      "Requirement already satisfied: cron-descriptor>=1.2.24 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.4.5)\n",
      "Requirement already satisfied: croniter>=2.0.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (5.0.1)\n",
      "Requirement already satisfied: cryptography>=41.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (42.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.2.15)\n",
      "Requirement already satisfied: dill>=0.2.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.3.7)\n",
      "Requirement already satisfied: flask-caching>=2.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.3.0)\n",
      "Requirement already satisfied: flask-session<0.6,>=0.4.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.5.0)\n",
      "Requirement already satisfied: flask-wtf>=1.1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.2.2)\n",
      "Requirement already satisfied: flask<2.3,>=2.2.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.2.5)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2023.10.0)\n",
      "Requirement already satisfied: google-re2>=1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.1.20240702)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (23.0.0)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.27.2)\n",
      "Requirement already satisfied: importlib_metadata>=6.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (7.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.0.1)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (4.19.2)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.2.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.6.0)\n",
      "Requirement already satisfied: linkify-it-py>=2.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.0.0)\n",
      "Requirement already satisfied: lockfile>=0.12.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=1.1.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.1.3)\n",
      "Requirement already satisfied: marshmallow-oneofschema>=2.0.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.1.1)\n",
      "Requirement already satisfied: mdit-py-plugins>=0.3.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.3.0)\n",
      "Requirement already satisfied: methodtools>=0.4.7 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.4.7)\n",
      "Requirement already satisfied: opentelemetry-api>=1.15.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp>=1.15.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (23.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.10.3)\n",
      "Requirement already satisfied: pendulum<4.0,>=2.1.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.0.0)\n",
      "Requirement already satisfied: pluggy>=1.5.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.5.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (5.9.0)\n",
      "Requirement already satisfied: pygments>=2.0.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.15.1)\n",
      "Requirement already satisfied: pyjwt>=2.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.4.0)\n",
      "Requirement already satisfied: python-daemon>=3.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.8.2)\n",
      "Requirement already satisfied: python-nvd3>=0.15.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.16.0)\n",
      "Requirement already satisfied: python-slugify>=5.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (5.0.2)\n",
      "Requirement already satisfied: requests-toolbelt>=0.4.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.0.0)\n",
      "Requirement already satisfied: rfc3339-validator>=0.1.4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.1.4)\n",
      "Requirement already satisfied: rich-argparse>=1.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.6.0)\n",
      "Requirement already satisfied: rich>=12.4.4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (13.3.5)\n",
      "Requirement already satisfied: setproctitle>=1.3.3 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.3.4)\n",
      "Requirement already satisfied: sqlalchemy<2.0,>=1.4.36 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.4.54)\n",
      "Requirement already satisfied: sqlalchemy-jsonfield>=1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.0.2)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (8.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.5.0)\n",
      "Requirement already satisfied: universal-pathlib!=0.2.4,>=0.2.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (0.2.5)\n",
      "Requirement already satisfied: werkzeug<3,>=2.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (2.2.3)\n",
      "Requirement already satisfied: apache-airflow-providers-common-compat in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.2.2)\n",
      "Requirement already satisfied: apache-airflow-providers-common-io in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.4.2)\n",
      "Requirement already satisfied: apache-airflow-providers-common-sql in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.20.0)\n",
      "Requirement already satisfied: apache-airflow-providers-fab>=1.0.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.5.1)\n",
      "Requirement already satisfied: apache-airflow-providers-ftp in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.11.1)\n",
      "Requirement already satisfied: apache-airflow-providers-http in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (4.13.3)\n",
      "Requirement already satisfied: apache-airflow-providers-imap in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.7.0)\n",
      "Requirement already satisfied: apache-airflow-providers-smtp in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (1.8.1)\n",
      "Requirement already satisfied: apache-airflow-providers-sqlite in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow) (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\personal\\anaconda3\\lib\\site-packages (from alembic<2.0,>=1.13.1->apache-airflow) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from alembic<2.0,>=1.13.1->apache-airflow) (4.9.0)\n",
      "Requirement already satisfied: flask-appbuilder==4.5.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.5.2)\n",
      "Requirement already satisfied: flask-login>=0.6.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.6.3)\n",
      "Requirement already satisfied: jmespath>=0.7.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: apispec<7,>=6.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apispec[yaml]<7,>=6.0.0->flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (6.7.1)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.4.6)\n",
      "Requirement already satisfied: click<9,>=8 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (8.1.7)\n",
      "Requirement already satisfied: email-validator>=1.0.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.2.0)\n",
      "Requirement already satisfied: Flask-Babel<3,>=1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.0.0)\n",
      "Requirement already satisfied: Flask-Limiter<4,>3 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.8.0)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.5.1)\n",
      "Requirement already satisfied: Flask-JWT-Extended<5.0.0,>=4.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.7.1)\n",
      "Requirement already satisfied: marshmallow<4,>=3.18.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.23.1)\n",
      "Requirement already satisfied: marshmallow-sqlalchemy<0.29.0,>=0.22.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.28.2)\n",
      "Requirement already satisfied: prison<1.0.0,>=0.2.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.2.1)\n",
      "Requirement already satisfied: sqlalchemy-utils<1,>=0.32.21 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.41.2)\n",
      "Requirement already satisfied: WTForms<4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.2.1)\n",
      "Requirement already satisfied: clickclick<21,>=1.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (20.10.2)\n",
      "Requirement already satisfied: PyYAML<7,>=5.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (6.0.1)\n",
      "Requirement already satisfied: inflection<0.6,>=0.3.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (0.5.1)\n",
      "Requirement already satisfied: pytz>2021.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from croniter>=2.0.2->apache-airflow) (2023.3.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from cryptography>=41.0.0->apache-airflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from deprecated>=1.2.13->apache-airflow) (1.14.1)\n",
      "Requirement already satisfied: cachelib<0.10.0,>=0.9.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from flask-caching>=2.0.0->apache-airflow) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\personal\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apache-airflow) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\personal\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apache-airflow) (1.0.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\personal\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apache-airflow) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from importlib_metadata>=6.5->apache-airflow) (3.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow) (0.10.6)\n",
      "Requirement already satisfied: uc-micro-py in c:\\users\\personal\\anaconda3\\lib\\site-packages (from linkify-it-py>=2.0.0->apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.1.0->apache-airflow) (0.1.0)\n",
      "Requirement already satisfied: wirerope>=0.4.7 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from methodtools>=0.4.7->apache-airflow) (0.4.8)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.28.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.28.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.68.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.28.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.28.2)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (5.28.3)\n",
      "Requirement already satisfied: tzdata>=2020.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2023.3)\n",
      "Requirement already satisfied: time-machine>=2.6.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2.16.0)\n",
      "Requirement already satisfied: setuptools>=62.4.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from python-daemon>=3.0.0->apache-airflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->apache-airflow) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from python-slugify>=5.0->apache-airflow) (1.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from sqlalchemy<2.0,>=1.4.36->apache-airflow) (3.0.1)\n",
      "Requirement already satisfied: more-itertools>=9.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow-providers-common-sql->apache-airflow) (10.1.0)\n",
      "Requirement already satisfied: sqlparse>=0.5.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow-providers-common-sql->apache-airflow) (0.5.2)\n",
      "Requirement already satisfied: aiohttp<3.11.0,>=3.9.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from apache-airflow-providers-http->apache-airflow) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from aiohttp<3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from aiohttp<3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from aiohttp<3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from aiohttp<3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.9.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\personal\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow) (2.21)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from email-validator>=1.0.5->flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.7.0)\n",
      "Requirement already satisfied: Babel>=2.3 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from Flask-Babel<3,>=1->flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.11.0)\n",
      "Requirement already satisfied: limits>=3.13 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.13.0)\n",
      "Requirement already satisfied: ordered-set<5,>4 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.1.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from opentelemetry-sdk~=1.28.2->opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (0.49b2)\n",
      "Requirement already satisfied: importlib-resources>=1.3 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from limits>=3.13->Flask-Limiter<4,>3->flask-appbuilder==4.5.2->apache-airflow-providers-fab>=1.0.2->apache-airflow) (6.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apache-airflow requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\personal\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\personal\\anaconda3\\lib\\site-packages (1.4.54)\n",
      "Requirement already satisfied: requests in c:\\users\\personal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\personal\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: pyspark in c:\\users\\personal\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: py4j in c:\\users\\personal\\anaconda3\\lib\\site-packages (0.10.9.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\personal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv sqlalchemy requests pandas pyspark py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark is working correctly with Spark version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\")\\\n",
    "            .appName(\"FinancedataETL\") \\\n",
    "            .config(\"spark.jars\", \"postgresql-42.7.3.jar\") \\\n",
    "            .config(\"spark.driver.extraClassPath\",  \"postgresql-42.7.3.jar\") \\\n",
    "            .config(\"spark.executor.extraClassPath\",  \"postgresql-42.7.3.jar\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\")\\\n",
    "            .config(\"spark.executor.memory\", \"8g\")\\\n",
    "            .config(\"spark.network.timeout\", \"600s\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Verify Spark session\n",
    "print(\"PySpark is working correctly with Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-1F00B3O:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FinancedataETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23530e9ecd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Knowing the spark UI\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\personal\\\\Downloads\\\\Finance_airflow\\\\dags\\\\modules'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.11.7)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"SPARK_HOME:\", os.getenv('SPARK_HOME'))\n",
    "print(\"JAVA_HOME:\", os.getenv('JAVA_HOME'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.11.7)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import findspark\n",
    "# Initialize Spark and environment variables\n",
    "findspark.init('C:/spark')\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Spark session with JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    master(\"local[*]\")\\\n",
    "    .appName(\"PostgresWriteTest\") \\\n",
    "    .config(\"spark.jars\", \"C:/spark/jars/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\",  \"C:/spark/jars/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\",  \"C:/spark/jars/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.network.timeout\", \"600s\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", 30), (\"Bob\", 35), (\"Cathy\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "output_path = r\"c:\\Users\\personal\\Downloads\\Finance_airflow\\dags\\modules\\test_output\"\n",
    "\n",
    "#df.write.mode(\"overwrite\").csv(output_path)\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "# JDBC connection details\n",
    "#jdbc_url = \"jdbc:postgresql://localhost:5432/Gold_Fintech\"\n",
    "#connection_properties = {\n",
    "    #\"user\": \"postgres\",\n",
    "    #\"password\": \"Chinwe31#\",\n",
    "    #\"driver\": \"org.postgresql.Driver\"\n",
    "#}\n",
    "\n",
    "# Try writing to PostgreSQL\n",
    "#try:\n",
    "    #df.write.jdbc(url=jdbc_url, table=\"alpha_vantage.test_data\", mode=\"append\", properties=connection_properties)\n",
    "    #print(\"New data loaded successfully into the database.\")\n",
    "#except Exception as e:\n",
    "    #print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (244035274.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    if not os.path.exists(\"c:\\Users\\personal\\Downloads\\Finance_airflow\\dags\\modules\\test.csv\"):\u001b[0m\n\u001b[1;37m                                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(\"c:\\Users\\personal\\Downloads\\Finance_airflow\\dags\\modules\\test.csv\"):\n",
    "    os.makedirs(\"c:\\Users\\personal\\Downloads\\Finance_airflow\\dags\\modules\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"PG_HOST\"))  # Check if variables are loaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/spark/jars/postgresql-42.7.3.jar\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"JDBCDriverTest\") \\\n",
    "    .set(\"spark.jars\", \"C:/spark/jars/postgresql-42.7.3.jar\")\n",
    "print(conf.get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1040.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 37\u001b[0m\n\u001b[0;32m     30\u001b[0m connection_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChinwe31#\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     }\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Directly write the new data to PostgreSQL using Spark's JDBC connector\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39mjdbc_url, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_vantage.test_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39mconnection_properties)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew data loaded successfully into the database.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mjdbc(url, table, jprop)\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1040.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import findspark\n",
    "from pyspark import SparkConf\n",
    "findspark.init('C:/spark')\n",
    "load_dotenv()\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinancedataETL\") \\\n",
    "    .config(\"spark.jars\", \"C:/spark/jars/postgresql-42.7.4.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\",  \"C:/spark/jars/postgresql-42.7.4.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\",  \"C:/spark/jars/postgresql-42.7.4.jar\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.network.timeout\", \"600s\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "data = [(\"Alice\", 30), (\"Bob\", 35), (\"Cathy\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "#output_path = \"c:\\\\Users\\\\personal\\\\Downloads\\\\Finance_airflow\\\\dags\\\\modules\\\\test.csv\"\n",
    "#df.write.mode(\"overwrite\").csv(output_path)\n",
    "#df.write.format(\"csv\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/Gold_Fintech\"\n",
    "connection_properties = {\n",
    "        \"user\": 'postgres',\n",
    "        \"password\": 'Chinwe31#',\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Directly write the new data to PostgreSQL using Spark's JDBC connector\n",
    "df.write.jdbc(url=jdbc_url, table=\"alpha_vantage.test_data\", mode=\"append\", properties=connection_properties)\n",
    "\n",
    "print(\"New data loaded successfully into the database.\")\n",
    "\n",
    "\n",
    "df.show()\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "Average Age by Department:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o108.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 3) (DESKTOP-1F00B3O executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#avg_age_df.show()\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Step 5: Save results to file system\u001b[39;00m\n\u001b[0;32m     41\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/pyspark_test_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m avg_age_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(output_path)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\personal\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o108.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 3) (DESKTOP-1F00B3O executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "import findspark\n",
    "from pyspark import SparkConf\n",
    "findspark.init('C:/spark')\n",
    "\n",
    "# Step 1: Initialize a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"PySpark Test Job\") \\\n",
    "    #.master(\"local[*]\") \\\n",
    "    #.getOrCreate()\n",
    "\n",
    "# Step 2: Create a sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Engineering\"),\n",
    "    (\"Bob\", 45, \"HR\"),\n",
    "    (\"Cathy\", 29, \"Finance\"),\n",
    "    (\"David\", 41, \"Engineering\"),\n",
    "    (\"Eva\", 37, \"HR\")\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Step 3: Perform basic transformations\n",
    "# Filter employees older than 30\n",
    "filtered_df = df.filter(col(\"Age\") > 30)\n",
    "\n",
    "# Group by department and calculate the average age\n",
    "avg_age_df = df.groupBy(\"Department\").agg(avg(\"Age\").alias(\"Average_Age\"))\n",
    "\n",
    "# Step 4: Show results\n",
    "print(\"Filtered DataFrame:\")\n",
    "#filtered_df.show()\n",
    "\n",
    "print(\"Average Age by Department:\")\n",
    "#avg_age_df.show()\n",
    "\n",
    "# Step 5: Save results to file system\n",
    "output_path = \"/tmp/pyspark_test_output\"\n",
    "avg_age_df.write.mode(\"overwrite\").csv(output_path)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        database=\"Gold_Fintech\",\n",
    "        user=\"postgres\",\n",
    "        password=\"Chinwe31#\"\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    connection.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
