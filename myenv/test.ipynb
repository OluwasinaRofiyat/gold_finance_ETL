{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached numpy-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.1 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.4-py2.py3-none-any.whl\n",
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10.tar.gz (385 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rita/Finance_ETL/myenv/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for psycopg2 \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[35 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/pool.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/_range.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/__init__.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/errors.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/extensions.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/errorcodes.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/sql.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/tz.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/extras.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/_json.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m copying lib/_ipaddress.py -> build/lib.linux-x86_64-cpython-312/psycopg2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'psycopg2._psycopg' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-312/psycopg\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -fPIC \"-DPSYCOPG_VERSION=2.9.10 (dt dec pq3 ext lo64)\" -DPSYCOPG_DEBUG=1 -DPG_VERSION_NUM=160006 -DHAVE_LO64=1 -DPSYCOPG_DEBUG=1 -I/home/rita/Finance_ETL/myenv/include -I/usr/include/python3.12 -I. -I/usr/include/postgresql -I/usr/include/postgresql/16/server -I/usr/include/libxml2 -c psycopg/adapter_asis.c -o build/temp.linux-x86_64-cpython-312/psycopg/adapter_asis.o -Wdeclaration-after-statement\n",
      "  \u001b[31m   \u001b[0m In file included from psycopg/adapter_asis.c:28:\n",
      "  \u001b[31m   \u001b[0m ./psycopg/psycopg.h:36:10: fatal error: libpq-fe.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m    36 | #include <libpq-fe.h>\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m It appears you are missing some prerequisite to build the package from source.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m You may install a binary package by installing 'psycopg2-binary' from PyPI.\n",
      "  \u001b[31m   \u001b[0m If you want to install psycopg2 from source, please install the packages\n",
      "  \u001b[31m   \u001b[0m required for the build and try again.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m For further information please check the 'doc/src/install.rst' file (also at\n",
      "  \u001b[31m   \u001b[0m <https://www.psycopg.org/docs/install.html>).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for psycopg2\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build psycopg2\n",
      "\u001b[31mERROR: Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark psycopg2 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching max date: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"database_name\" does not exist\n",
      "\n",
      "        date  daily_open  daily_high  daily_low  daily_close  daily_volume  \\\n",
      "0 2024-12-23       364.8     367.800      362.4        366.0       5714705   \n",
      "1 2024-12-20       367.8     368.900      364.6        366.4      40046434   \n",
      "2 2024-12-19       371.2     372.088      368.2        368.5      22673910   \n",
      "3 2024-12-18       373.3     375.700      371.0        372.9      22537119   \n",
      "4 2024-12-17       371.0     375.400      370.5        374.0      11827790   \n",
      "\n",
      "  last_refreshed Output Size   Time Zone  \\\n",
      "0     2024-12-23   Full size  US/Eastern   \n",
      "1     2024-12-23   Full size  US/Eastern   \n",
      "2     2024-12-23   Full size  US/Eastern   \n",
      "3     2024-12-23   Full size  US/Eastern   \n",
      "4     2024-12-23   Full size  US/Eastern   \n",
      "\n",
      "                                         Description    symbol  \n",
      "0  Daily Prices (open, high, low, close) and Volumes  TSCO.LON  \n",
      "1  Daily Prices (open, high, low, close) and Volumes  TSCO.LON  \n",
      "2  Daily Prices (open, high, low, close) and Volumes  TSCO.LON  \n",
      "3  Daily Prices (open, high, low, close) and Volumes  TSCO.LON  \n",
      "4  Daily Prices (open, high, low, close) and Volumes  TSCO.LON  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the max date from the PostgreSQL staging table\n",
    "def get_max_date_from_staging(connection_string, table_name):\n",
    "    \"\"\"Fetch the maximum date from the staging table in PostgreSQL\"\"\"\n",
    "    query = f\"SELECT MAX(date) FROM {table_name};\"\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        max_date = cursor.fetchone()[0]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # If max_date is None, return a default old date (e.g., 2000-01-01)\n",
    "        return max_date if max_date else datetime(2000, 1, 1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching max date: {e}\")\n",
    "        return datetime(2000, 1, 1)\n",
    "\n",
    "def fetch_alpha_vantage_data(symbol, api_key, max_date_from_db):\n",
    "    \"\"\"\n",
    "    Fetch daily stock data for a given symbol from the Alpha Vantage API and return as a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    symbol (str): Stock ticker symbol (e.g., \"AAPL\" for Apple Inc.).\n",
    "    api_key (str): API key for Alpha Vantage.\n",
    "    max_date_from_db (datetime): Maximum date from the staging table to control data extraction.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with the time series data for the symbol.\n",
    "    \"\"\"\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting metadata\n",
    "    meta_data = data.get('Meta Data', {})\n",
    "    description = meta_data.get('1. Information', '')\n",
    "    last_refreshed = meta_data.get('3. Last Refreshed', '')\n",
    "    output_size = meta_data.get('4. Output Size', 'N/A')\n",
    "    time_zone = meta_data.get('5. Time Zone', 'N/A')\n",
    "    symbol = meta_data.get('2. Symbol', '')\n",
    "    \n",
    "    # Initializing empty lists for time series data\n",
    "    dates = []\n",
    "    opens = []\n",
    "    highs = []\n",
    "    lows = []\n",
    "    closes = []\n",
    "    volumes = []\n",
    "    \n",
    "    # Iterate over daily data\n",
    "    for date, daily_data in data.get('Time Series (Daily)', {}).items():\n",
    "        # Convert date to datetime\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')\n",
    "        \n",
    "        # Only add data if the date is greater than the max date from the staging table\n",
    "        if date > max_date_from_db:\n",
    "            dates.append(date)\n",
    "            opens.append(float(daily_data.get('1. open', 0)))\n",
    "            highs.append(float(daily_data.get('2. high', 0)))\n",
    "            lows.append(float(daily_data.get('3. low', 0)))\n",
    "            closes.append(float(daily_data.get('4. close', 0)))\n",
    "            volumes.append(int(daily_data.get('5. volume', 0)))\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'daily_open': opens,\n",
    "        'daily_high': highs,\n",
    "        'daily_low': lows,\n",
    "        'daily_close': closes,\n",
    "        'daily_volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df['last_refreshed'] = last_refreshed\n",
    "    df['Output Size'] = output_size\n",
    "    df['Time Zone'] = time_zone\n",
    "    df['Description'] = description\n",
    "    df['symbol'] = symbol  \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Define your PostgreSQL connection string\n",
    "connection_string = \"postgresql://postgres:Chinwe31#@localhost:5432/database_name\"\n",
    "\n",
    "# Define the table name in the staging area\n",
    "table_name = \"alpha_vantage.staging_finance_data\"\n",
    "\n",
    "# Get the maximum date from the PostgreSQL staging table\n",
    "max_date_from_db = get_max_date_from_staging(connection_string, table_name)\n",
    "\n",
    "# Define the list of symbols and your API key\n",
    "symbols = ['TSCO.LON', 'IBM', 'MBG.DEX', 'SHOP.TRT']\n",
    "api_key = \"ML7BZYF38ZPZLHR4\"\n",
    "\n",
    "# Empty list to collect all dataframes\n",
    "df_list = []\n",
    "\n",
    "# Looping through the symbols and fetching data for each\n",
    "for symbol in symbols:\n",
    "    df_symbol = fetch_alpha_vantage_data(symbol, api_key, max_date_from_db)\n",
    "    df_list.append(df_symbol)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined dataframe\n",
    "print(df_combined.head())\n",
    "\n",
    "\n",
    "\n",
    "# Save the combined dataframe to CSV\n",
    "df_combined.to_csv('./alpha_vantage.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.4-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/24 17:52:56 WARN Utils: Your hostname, DESKTOP-1F00B3O resolves to a loopback address: 127.0.1.1; using 172.21.118.133 instead (on interface eth0)\n",
      "24/12/24 17:52:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 17:53:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/24 17:53:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, Output Size, Time Zone, Description, symbol\n",
      " Schema: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, output_size, time_zone, description, symbol\n",
      "Expected: output_size but found: Output Size\n",
      "CSV file: file:///home/rita/Finance_ETL/myenv/myenv/alpha_vantage.csv\n",
      "24/12/24 17:53:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, Output Size, Time Zone, Description, symbol\n",
      " Schema: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, output_size, time_zone, description, symbol\n",
      "Expected: output_size but found: Output Size\n",
      "CSV file: file:///home/rita/Finance_ETL/myenv/myenv/alpha_vantage.csv\n",
      "24/12/24 17:53:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, Output Size, Time Zone, Description, symbol\n",
      " Schema: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, output_size, time_zone, description, symbol\n",
      "Expected: output_size but found: Output Size\n",
      "CSV file: file:///home/rita/Finance_ETL/myenv/myenv/alpha_vantage.csv\n",
      "24/12/24 17:53:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, Output Size, Time Zone, Description, symbol\n",
      " Schema: date, daily_open, daily_high, daily_low, daily_close, daily_volume, last_refreshed, output_size, time_zone, description, symbol\n",
      "Expected: output_size but found: Output Size\n",
      "CSV file: file:///home/rita/Finance_ETL/myenv/myenv/alpha_vantage.csv\n",
      "24/12/24 17:53:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/24 17:53:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------+-----------+------------+--------------+-----------+----------+--------------------+------+-------------------+\n",
      "|      date|daily_open|daily_high|daily_low|daily_close|daily_volume|last_refreshed|output_size| time_zone|         description|symbol|       daily_return|\n",
      "+----------+----------+----------+---------+-----------+------------+--------------+-----------+----------+--------------------+------+-------------------+\n",
      "|2000-01-03|    112.44|     116.0|   111.87|      116.0|    10347700|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|               NULL|\n",
      "|2000-01-04|     114.0|     114.5|   110.87|     112.06|     8227800|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|-3.9399999999999977|\n",
      "|2000-01-05|    112.94|    119.75|   112.12|      116.0|    12733200|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM| 3.9399999999999977|\n",
      "|2000-01-06|     118.0|    118.94|    113.5|      114.0|     7971900|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|               -2.0|\n",
      "|2000-01-07|    117.25|    117.94|   110.62|      113.5|    11856700|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|               -0.5|\n",
      "|2000-01-10|    117.25|    119.37|   115.37|      118.0|     8540500|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|                4.5|\n",
      "|2000-01-11|    117.87|    121.12|   116.62|      119.0|     7873300|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|                1.0|\n",
      "|2000-01-12|    119.62|     122.0|   118.25|      119.5|     6803800|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|                0.5|\n",
      "|2000-01-13|    119.94|     121.0|   115.75|     118.25|     8489700|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|              -1.25|\n",
      "|2000-01-14|    120.94|    123.31|    117.5|     119.62|    10956600|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM| 1.3700000000000045|\n",
      "|2000-01-18|    119.69|    119.75|    115.0|     115.75|     7643900|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|-3.8700000000000045|\n",
      "|2000-01-19|    115.56|     122.0|   112.69|      119.5|     8634500|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|               3.75|\n",
      "|2000-01-20|     123.0|    124.75|    119.0|      119.0|    17783400|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|               -0.5|\n",
      "|2000-01-21|    121.87|     123.0|   119.94|      121.5|     7868700|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|                2.5|\n",
      "|2000-01-24|    121.87|    122.87|   116.56|      121.5|     6499200|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|                0.0|\n",
      "|2000-01-25|    116.75|     119.5|    116.0|     119.12|     6936900|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|-2.3799999999999955|\n",
      "|2000-01-26|    119.06|    119.94|    116.0|     116.75|     4895100|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|-2.3700000000000045|\n",
      "|2000-01-27|     118.0|    118.44|   111.62|      113.5|     8324600|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|              -3.25|\n",
      "|2000-01-28|    112.75|    114.19|   110.06|     111.56|     6669400|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM|-1.9399999999999977|\n",
      "|2000-01-31|    111.37|    112.81|   109.62|     112.25|     6202700|    2024-12-23|  Full size|US/Eastern|Daily Prices (ope...|   IBM| 0.6899999999999977|\n",
      "+----------+----------+----------+---------+-----------+------------+--------------+-----------+----------+--------------------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, StringType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "def transform_and_write_to_csv():\n",
    "    \"\"\"\n",
    "    Transforms the input pandas DataFrame and writes it to CSV as a Spark DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - pandas_df (pd.DataFrame): Input pandas DataFrame with time series data.\n",
    "    - output_path (str): Path to save the transformed Spark DataFrame in CSV format.\n",
    "    \"\"\"\n",
    "    # Convert the 'date' column to datetime (DateType) in pandas DataFrame\n",
    "    #pandas_df['date'] = pd.to_datetime(pandas_df['date'])\n",
    "    #pandas_df['last_refreshed'] = pd.to_datetime(pandas_df['last_refreshed'])\n",
    "    \n",
    "    # Define the schema for the Spark DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"daily_open\", DoubleType(), True),\n",
    "        StructField(\"daily_high\", DoubleType(), True),\n",
    "        StructField(\"daily_low\", DoubleType(), True),\n",
    "        StructField(\"daily_close\", DoubleType(), True),\n",
    "        StructField(\"daily_volume\", IntegerType(), True),\n",
    "        StructField(\"last_refreshed\", StringType(), True),\n",
    "        StructField(\"output_size\", StringType(), True),\n",
    "        StructField(\"time_zone\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"symbol\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"Alpha Vantage Data Transformation\").getOrCreate()\n",
    "\n",
    "    # Convert the pandas DataFrame to a Spark DataFrame using the defined schema\n",
    "    spark_df = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(\"./alpha_vantage.csv\")\n",
    "    #spark_df = spark.createDataFrame(pandas_df, schema=schema)\n",
    "    \n",
    "    # Convert 'date' column to DateType and sort by date\n",
    "    spark_df = spark_df.withColumn('date', F.to_date(F.col('date'))).orderBy('date')\n",
    "    \n",
    "    # Define a window specification for calculating daily return\n",
    "    window_spec = Window.orderBy('date')\n",
    "    \n",
    "    # Calculate daily returns (difference in closing price)\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'daily_return',\n",
    "        F.col('daily_close') - F.lag('daily_close', 1).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # Write the Spark DataFrame to CSV\n",
    "    spark_df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path)\n",
    "    spark_df.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `all_data_df` is the pandas DataFrame containing your data\n",
    "#df_all =  spark.read.format(\"csv\").option(\"header\",True).load(\"./alpha_vantage.csv\")\n",
    "\n",
    "output_path = './alpha_vantage_transformed'\n",
    "transform_and_write_to_csv()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
