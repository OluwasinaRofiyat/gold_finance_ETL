{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: sqlalchemy in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.39)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: pyspark in c:\\programdata\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.10.9.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv sqlalchemy requests pandas pyspark py4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType, TimestampType\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import findspark\n",
    "from pyspark import SparkConf\n",
    "findspark.init('C:/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark is working correctly with Spark version: 3.4.4\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\")\\\n",
    "            .appName(\"FinancedataETL\") \\\n",
    "            .config(\"spark.jars\", \"postgresql-42.7.3.jar\") \\\n",
    "            .config(\"spark.driver.extraClassPath\",  \"postgresql-42.7.3.jar\") \\\n",
    "            .config(\"spark.executor.extraClassPath\",  \"postgresql-42.7.3.jar\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\")\\\n",
    "            .config(\"spark.executor.memory\", \"8g\")\\\n",
    "            .config(\"spark.network.timeout\", \"600s\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Verify Spark session\n",
    "print(\"PySpark is working correctly with Spark version:\", spark.version)\n",
    "\n",
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FinancedataETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2709f054450>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Knowing the spark UI\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API request function\n",
    "def fetch_data(symbol, api_key='KA3DQ2SRS8MOUN8J', output_size='full'):\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize={output_size}&apikey={api_key}'\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "# Asynchronous request function\n",
    "async def fetch_data_async(symbol, api_key='KA3DQ2SRS8MOUN8J', output_size='full'):\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize={output_size}&apikey={api_key}'\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.json()\n",
    "\n",
    "# Fetch data for all symbols\n",
    "async def fetch_all_data(symbols):\n",
    "    tasks = [fetch_data_async(symbol) for symbol in symbols]\n",
    "    return await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"daily_open\", FloatType(), True),\n",
    "    StructField(\"daily_high\", FloatType(), True),\n",
    "    StructField(\"daily_low\", FloatType(), True),\n",
    "    StructField(\"daily_close\", FloatType(), True),\n",
    "    StructField(\"daily_volume\", IntegerType(), True),\n",
    "    StructField(\"last_refreshed\", DateType(), True),\n",
    "    StructField(\"output_size\", StringType(), True),\n",
    "    StructField(\"time_zone\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Process data and accumulate in a Pandas DataFrame first\n",
    "all_rows = []\n",
    "\n",
    "async def process_data(symbols):\n",
    "    responses = await fetch_all_data(symbols)\n",
    "    for symbol, data in zip(symbols, responses):\n",
    "        meta_data = data.get('Meta Data', {})\n",
    "        description = meta_data.get('1. Information', '').split(' ')[0] + ' ' + meta_data.get('1. Information', '').split(' ')[1]\n",
    "        last_refreshed = meta_data.get('3. Last Refreshed', None)\n",
    "        output_size = meta_data.get('4. Output Size', 'N/A')\n",
    "        time_zone = meta_data.get('5. Time Zone', 'N/A')\n",
    "        \n",
    "        # Extract time series data\n",
    "        time_series = data.get('Time Series (Daily)', {})\n",
    "        for date, daily_data in time_series.items():\n",
    "              # Convert `date` string to `datetime.date` object\n",
    "            date_obj = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "\n",
    "            # Check the format of `last_refreshed`\n",
    "            if last_refreshed and len(last_refreshed) == 10:\n",
    "                last_refreshed_dt = datetime.strptime(last_refreshed, '%Y-%m-%d')\n",
    "            elif last_refreshed:\n",
    "                last_refreshed_dt = datetime.strptime(last_refreshed, '%Y-%m-%d')\n",
    "            else:\n",
    "                last_refreshed_dt = None\n",
    "            all_rows.append({\n",
    "                'date': date_obj,\n",
    "                'daily_open': float(daily_data['1. open']),\n",
    "                'daily_high': float(daily_data['2. high']),\n",
    "                'daily_low': float(daily_data['3. low']),\n",
    "                'daily_close': float(daily_data['4. close']),\n",
    "                'daily_volume': int(daily_data['5. volume']),\n",
    "                'last_refreshed': last_refreshed_dt,\n",
    "                'output_size': output_size,\n",
    "                'time_zone': time_zone,\n",
    "                'description': description,\n",
    "                'symbol': symbol\n",
    "            })\n",
    "\n",
    "symbols = ['TSCO.LON', 'IBM', 'MBG.DEX', 'SHOP.TRT']\n",
    "await process_data(symbols)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_pandas = pd.DataFrame(all_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If there is data, convert to Spark DataFrame\n",
    "if not df_pandas.empty:\n",
    "    df = spark.createDataFrame(df_pandas, schema=schema)\n",
    "else:\n",
    "    print(\"No data found to load into Spark DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dataset\n",
    "#df=df.repartition(4)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, daily_open: float, daily_high: float, daily_low: float, daily_close: float, daily_volume: int, last_refreshed: date, output_size: string, time_zone: string, description: string, symbol: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a 5-day window spec (based on row count instead of date range)\n",
    "window_spec = Window.orderBy(\"date\").rowsBetween(-4, 0)\n",
    "\n",
    "# Calculate the 5-day moving average for the \"daily_close\" column\n",
    "df = df.withColumn(\"5_day_moving_avg\", F.round(F.avg(\"daily_close\").over(window_spec), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+----------+---------+-----------+------------+----------------+------------+------+\n",
      "|      date|last_refreshed|daily_open|daily_high|daily_low|daily_close|daily_volume|5_day_moving_avg| description|symbol|\n",
      "+----------+--------------+----------+----------+---------+-----------+------------+----------------+------------+------+\n",
      "|1999-12-03|    2024-11-15|    109.81|    112.87|   107.94|     111.87|    14680300|          105.56|Daily Prices|   IBM|\n",
      "|1999-12-06|    2024-11-15|     113.0|     116.5|    112.5|      116.0|     9928300|          107.92|Daily Prices|   IBM|\n",
      "|1999-12-07|    2024-11-15|     117.0|    119.19|   115.44|     116.62|    11326100|          110.64|Daily Prices|   IBM|\n",
      "|1999-12-08|    2024-11-15|    116.25|     121.0|    115.5|     118.28|     8139800|          113.61|Daily Prices|   IBM|\n",
      "|1999-12-09|    2024-11-15|     120.5|    122.12|   112.25|     113.37|    16643000|          115.23|Daily Prices|   IBM|\n",
      "|1999-12-21|    2024-11-15|     108.5|    110.12|   108.06|     110.12|     4773500|           109.1|Daily Prices|   IBM|\n",
      "|2000-01-03|    2024-11-15|    112.44|     116.0|   111.87|      116.0|    10347700|          110.29|Daily Prices|   IBM|\n",
      "|2000-01-04|    2024-11-15|     114.0|     114.5|   110.87|     112.06|     8227800|          110.74|Daily Prices|   IBM|\n",
      "|2000-01-05|    2024-11-15|    112.94|    119.75|   112.12|      116.0|    12733200|          112.14|Daily Prices|   IBM|\n",
      "|2000-01-06|    2024-11-15|     118.0|    118.94|    113.5|      114.0|     7971900|          113.19|Daily Prices|   IBM|\n",
      "|2000-01-07|    2024-11-15|    117.25|    117.94|   110.62|      113.5|    11856700|          114.31|Daily Prices|   IBM|\n",
      "|2000-01-10|    2024-11-15|    117.25|    119.37|   115.37|      118.0|     8540500|          114.71|Daily Prices|   IBM|\n",
      "|2000-01-11|    2024-11-15|    117.87|    121.12|   116.62|      119.0|     7873300|           116.1|Daily Prices|   IBM|\n",
      "|2000-01-12|    2024-11-15|    119.62|     122.0|   118.25|      119.5|     6803800|           116.8|Daily Prices|   IBM|\n",
      "|2000-01-13|    2024-11-15|    119.94|     121.0|   115.75|     118.25|     8489700|          117.65|Daily Prices|   IBM|\n",
      "|2000-01-14|    2024-11-15|    120.94|    123.31|    117.5|     119.62|    10956600|          118.87|Daily Prices|   IBM|\n",
      "|2000-01-18|    2024-11-15|    119.69|    119.75|    115.0|     115.75|     7643900|          118.42|Daily Prices|   IBM|\n",
      "|2000-01-19|    2024-11-15|    115.56|     122.0|   112.69|      119.5|     8634500|          118.52|Daily Prices|   IBM|\n",
      "|2000-01-20|    2024-11-15|     123.0|    124.75|    119.0|      119.0|    17783400|          118.42|Daily Prices|   IBM|\n",
      "|2000-01-21|    2024-11-15|    121.87|     123.0|   119.94|      121.5|     7868700|          119.07|Daily Prices|   IBM|\n",
      "+----------+--------------+----------+----------+---------+-----------+------------+----------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting specific columns and filter rows \n",
    "from pyspark.sql.functions import col\n",
    "df_filtered = df.select(\"date\",\"last_refreshed\",\"daily_open\",\"daily_high\",\"daily_low\",\"daily_close\", \"daily_volume\", \"5_day_moving_avg\",\"description\",\"symbol\") \n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------------+----------------+\n",
      "|  symbol|avg_daily_open|avg_daily_close|avg_daily_volume|\n",
      "+--------+--------------+---------------+----------------+\n",
      "|TSCO.LON|        298.33|          298.2|   2.174187031E7|\n",
      "|     IBM|        150.59|         150.68|      5141763.07|\n",
      "|SHOP.TRT|         718.3|         717.61|      3043116.08|\n",
      "+--------+--------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by symbol and calculating the average daily close \n",
    "df_grouped = df_filtered.groupBy(\"symbol\").agg(F.round(F.avg(\"daily_open\"), 2).alias(\"avg_daily_open\"), F.round(F.avg(\"daily_close\"), 2).alias(\"avg_daily_close\"), F.round(F.avg(\"daily_volume\"), 2).alias(\"avg_daily_volume\"))\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last loaded date: Column<'to_date(TIMESTAMP '2024-11-08 00:00:00')'>\n",
      "New data range: 2024-11-11 to 2024-11-15\n",
      "New data loaded successfully into the database.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine, Integer, Float, String, DateTime, TIMESTAMP\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Creating a PostgreSQL connection\n",
    "engine = create_engine(\n",
    "    'postgresql://{user}:{password}@{host}:{port}/{database}'.format(\n",
    "        user=os.getenv('PG_USER'),\n",
    "        password=os.getenv('PG_PASSWORD'),\n",
    "        host=os.getenv('PG_HOST'),\n",
    "        port=os.getenv('PG_PORT'),\n",
    "        database=os.getenv('PG_DATABASE')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Function to get the last inserted date from the database, ensuring incremental loading\n",
    "def get_last_loaded_date(engine):\n",
    "    query = \"SELECT MAX(date) FROM daily_finance;\"\n",
    "    result = engine.execute(query).fetchone()\n",
    "    return result[0] if result[0] is not None else None\n",
    "\n",
    "# Get the last loaded date from the database\n",
    "last_loaded_date = get_last_loaded_date(engine)\n",
    "\n",
    "# Convert 'date' column to datetime in PySpark (ensure consistency)\n",
    "df_filtered = df_filtered.withColumn('date', to_date(col('date')))\n",
    "\n",
    "# Filters the DataFrame to include only new data (dates greater than the last loaded date)\n",
    "if last_loaded_date:\n",
    "    # Convert last_loaded_date to the same format as PySpark date\n",
    "    from pyspark.sql.functions import lit\n",
    "    last_loaded_date = to_date(lit(last_loaded_date))\n",
    "\n",
    "    # Filter for new data (only rows with a 'date' greater than the last loaded date)\n",
    "    new_data = df_filtered.filter(col('date') > last_loaded_date)\n",
    "\n",
    "    # Debugging information for date range\n",
    "    if new_data.count() > 0:\n",
    "        print(\"Last loaded date:\", last_loaded_date)\n",
    "        print(\"New data range:\", new_data.agg({\"date\": \"min\"}).collect()[0][0], \"to\", new_data.agg({\"date\": \"max\"}).collect()[0][0])\n",
    "else:\n",
    "    # If there is no data loaded yet, load all\n",
    "    new_data = df_filtered\n",
    "\n",
    "# Check if there is any new data to insert\n",
    "if new_data.count() > 0:\n",
    "    # Define JDBC URL and properties for PostgreSQL connection\n",
    "    jdbc_url = f\"jdbc:postgresql://{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DATABASE')}\"\n",
    "    connection_properties = {\n",
    "        \"user\": os.getenv('PG_USER'),\n",
    "        \"password\": os.getenv('PG_PASSWORD'),\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Directly write the new data to PostgreSQL using Spark's JDBC connector\n",
    "    new_data.write.jdbc(url=jdbc_url, table=\"daily_finance\", mode=\"append\", properties=connection_properties)\n",
    "\n",
    "    print(\"New data loaded successfully into the database.\")\n",
    "else:\n",
    "    print(\"No new data to load, just historic data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
